# Core prompt generation logic
import re
from pathlib import Path

def parse_template(template_path: Path) -> list[str]:
    """
    Reads a template file and parses it to find placeholders.

    Args:
        template_path: The path to the template file.

    Returns:
        A list of unique placeholder names found in the template.
    """
    try:
        content = template_path.read_text()
        
        # First, try to find {{placeholder}} format
        placeholders = re.findall(r"\{\{([a-zA-Z0-9_]+)\}\}", content)
        
        # If no {{}} placeholders found, look for **[FILL IN: ...] format with section headers
        if not placeholders:
            placeholders = []
            lines = content.split('\n')
            current_section = None
            
            for line in lines:
                # Look for section headers (## Header Name)
                header_match = re.match(r'^## (.+)$', line.strip())
                if header_match:
                    # Convert section header to placeholder name
                    section_name = header_match.group(1)
                    # Remove special characters and convert to snake_case
                    section_name = re.sub(r'[^\w\s]', '', section_name)
                    section_name = re.sub(r'\s+', '_', section_name.strip().lower())
                    current_section = section_name
                
                # Look for FILL IN instructions
                elif '**[FILL IN:' in line and current_section:
                    # Use the current section name as the placeholder
                    if current_section and current_section not in placeholders:
                        placeholders.append(current_section)
        
        return sorted(list(set(placeholders)))
    except FileNotFoundError:
        return []

import os
import json
import google.generativeai as genai

def assemble_prompt(template_content: str, user_data: dict, meta_prompt_path: Path) -> str:
    """
    Assembles the final prompt by sending the template, user data, and a meta-prompt to the AI.

    Args:
        template_content: The content of the prompt template.
        user_data: A dictionary containing the user's answers.
        meta_prompt_path: The path to the meta-prompt file.

    Returns:
        The final, assembled prompt generated by the AI.
    """
    try:
        # Configure the API key from an environment variable
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable not set.")
        genai.configure(api_key=api_key)

        # Load the meta-prompt
        meta_prompt = meta_prompt_path.read_text()

        # Format the final prompt for the AI
        user_data_json = json.dumps(user_data, indent=4)
        formatted_prompt = meta_prompt.replace("{{template}}", template_content).replace("{{user_data}}", user_data_json)

        # Call the Gemini API
        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(formatted_prompt)

        return response.text

    except Exception as e:
        return f"An error occurred: {e}"
